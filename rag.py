import os
import hashlib
import json
import streamlit as st
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
#from recognition_model import MedicineRecognitionModel
from PIL import Image

# üîê Load API key

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("‚ùå Kh√¥ng t√¨m th·∫•y API key trong file .env")
    st.stop()


# C·∫•u h√¨nh

DATA_PATH = "text.txt"           # Database thu·ªëc b·∫Øc
FAISS_PATH = "faiss_index"       # Th∆∞ m·ª•c FAISS
HASH_PATH = os.path.join(FAISS_PATH, "data_hash.json")


# H√†m ti·ªán √≠ch

@st.cache_resource
def load_recognition_model():
    return #MedicineRecognitionModel("model/thuoc_recognition.pth")

recognition_model = load_recognition_model()

def compute_md5(text):
    """T√≠nh hash MD5 c·ªßa ƒëo·∫°n vƒÉn b·∫£n."""
    return hashlib.md5(text.encode("utf-8")).hexdigest()

def compute_file_hash(filepath):
    """T√≠nh hash t·ªïng c·ªßa file text.txt."""
    with open(filepath, "r", encoding="utf-8") as f:
        return hashlib.md5(f.read().encode("utf-8")).hexdigest()



# Incremental FAISS Loader

def load_or_update_faiss(data_path=DATA_PATH, db_path=FAISS_PATH):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
    os.makedirs(db_path, exist_ok=True)

    # ƒê·ªçc n·ªôi dung file text
    with open(data_path, "r", encoding="utf-8") as f:
        full_text = f.read()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    chunks = splitter.split_text(full_text)

    # Load hash c≈© (n·∫øu c√≥)
    old_hashes = {}
    if os.path.exists(HASH_PATH):
        with open(HASH_PATH, "r", encoding="utf-8") as f:
            old_hashes = json.load(f)

    # T√≠nh hash m·ªõi c·ªßa t·ª´ng chunk
    new_chunks = []
    new_hashes = {}
    for chunk in chunks:
        h = compute_md5(chunk)
        new_hashes[h] = True
        if h not in old_hashes:
            new_chunks.append(chunk)

    # N·∫øu ch∆∞a c√≥ FAISS ‚Üí t·∫°o m·ªõi
    if not os.path.exists(os.path.join(db_path, "index.faiss")):
        st.info("üß† T·∫°o m·ªõi FAISS database...")
        vector_store = FAISS.from_texts(chunks, embedding=embeddings)
    else:
        # Load FAISS c≈©
        vector_store = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        # N·∫øu c√≥ chunk m·ªõi th√¨ th√™m v√†o
        if new_chunks:
            st.info(f"üîÑ Ph√°t hi·ªán {len(new_chunks)} ƒëo·∫°n m·ªõi ‚Üí c·∫≠p nh·∫≠t FAISS...")
            vector_store.add_texts(new_chunks)
        else:
            st.success("‚úÖ Kh√¥ng c√≥ thay ƒë·ªïi m·ªõi trong database.")

    # Save FAISS + hash
    vector_store.save_local(db_path)
    with open(HASH_PATH, "w", encoding="utf-8") as f:
        json.dump(new_hashes, f, ensure_ascii=False, indent=2)

    st.success("‚úÖ FAISS ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t.")
    return vector_store


# RAG QA Chain
def create_qa_chain(vector_store):
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})

    prompt_template = """
    B·∫°n l√† m·ªôt **chuy√™n gia y h·ªçc c·ªï truy·ªÅn Vi·ªát Nam** am hi·ªÉu d∆∞·ª£c li·ªáu v√† b√†i thu·ªëc b·∫Øc.
    Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc k·ªπ ng·ªØ c·∫£nh v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin c√≥ trong ƒë√≥.

    üîπ Quy t·∫Øc tr·∫£ l·ªùi:
    1. Gi·∫£i th√≠ch ng·∫Øn g·ªçn tr∆∞·ªõc (2‚Äì3 c√¢u t·ªïng qu√°t).
    2. Li·ªát k√™ chi ti·∫øt c√°c **b√†i thu·ªëc** li√™n quan (n·∫øu c√≥), g·ªìm:
       - **T√™n b√†i thu·ªëc:** ...
       - **Th√†nh ph·∫ßn:** ...
       - **C√°ch d√πng:** ...
       - **Ch·ªâ ƒë·ªãnh:** ...
       - **Tr√≠ch d·∫´n:** ...
    3. N·∫øu c√≥ nhi·ªÅu b√†i thu·ªëc, s·∫Øp x·∫øp theo ƒë·ªô ph√π h·ª£p cao nh·∫•t.
    4. N·∫øu kh√¥ng t√¨m th·∫•y, tr·∫£ l·ªùi: "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu."
    5. Cu·ªëi c√πng, th√™m ghi ch√∫: *(Th√¥ng tin ch·ªâ mang t√≠nh tham kh·∫£o, kh√¥ng thay th·∫ø t∆∞ v·∫•n y khoa).*

    -----------------
    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}
    -----------------
    Tr·∫£ l·ªùi:
    """

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    return qa_chain



# UI Streamlit

def main():
    st.set_page_config(page_title="Chatbot Thu·ªëc B·∫Øc", page_icon="üåø", layout="wide")
    st.title("üåø Chatbot Y h·ªçc C·ªï truy·ªÅn ‚Äì Ph√¢n t√≠ch & G·ª£i √Ω b√†i thu·ªëc")

    # 1Ô∏è‚É£ T·∫£i v√† ki·ªÉm tra d·ªØ li·ªáu
    if not os.path.exists(DATA_PATH):
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file {DATA_PATH}. H√£y ƒë·∫∑t n√≥ c√πng th∆∞ m·ª•c ·ª©ng d·ª•ng.")
        st.stop()

    st.info("üìÇ ƒêang t·∫£i ho·∫∑c c·∫≠p nh·∫≠t FAISS database t·ª´ text.txt...")
    vector_store = load_or_update_faiss(DATA_PATH, FAISS_PATH)
    

    # 2Ô∏è‚É£ Nh·∫≠p c√¢u h·ªèi
    st.markdown("### üí¨ H·ªèi v·ªÅ b√†i thu·ªëc / tri·ªáu ch·ª©ng")
    user_question = st.text_input("V√≠ d·ª•: 'B√†i thu·ªëc n√†o tr·ªã m·∫•t ng·ªß c√≥ v·ªã k·ª∑ t·ª≠ v√† ho√†ng k·ª≥?'")

    if user_question:
        with st.spinner("üßò‚Äç‚ôÇÔ∏è ƒêang tham kh·∫£o b√†i thu·ªëc trong c∆° s·ªü d·ªØ li·ªáu..."):
            qa_chain = create_qa_chain(vector_store)
            result = qa_chain({"query": user_question})

        st.success("‚úÖ ƒê√£ ho√†n th√†nh qu√° tr√¨nh tham kh·∫£o b√†i thu·ªëc.")
        st.markdown("### üß† K·∫øt qu·∫£ ph√¢n t√≠ch:")
        st.write(result["result"])
        
    
# Run
if __name__ == "__main__":
    main()
