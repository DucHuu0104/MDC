import os
import hashlib
import json
import io
import streamlit as st
from dotenv import load_dotenv
from PIL import Image
from ultralytics import YOLO
# LangChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
#from your_module import load_or_update_faiss, create_qa_chain, MedicineRecognitionModel
# ======================================================
# üîç Gi·∫£ ƒë·ªãnh b·∫°n c√≥ s·∫µn model nh·∫≠n d·∫°ng thu·ªëc
# recognition_model.py
# class MedicineRecognitionModel:
#     def __init__(self, model_path): ...
#     def predict(self, image): return "Ho√†ng k·ª≥"
# ======================================================
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Load API key
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("‚ùå Kh√¥ng t√¨m th·∫•y API key trong file .env")
    st.stop()

# C·∫•u h√¨nh
DATA_PATH = "text.txt"
FAISS_PATH = "faiss_index"
HASH_PATH = os.path.join(FAISS_PATH, "data_hash.json")

# =============================
# üîπ H√†m ti·ªán √≠ch
# =============================
def compute_md5(text):
    return hashlib.md5(text.encode("utf-8")).hexdigest()

# =============================
# üîπ FAISS loader/update
# =============================
def load_or_update_faiss(data_path=DATA_PATH, db_path=FAISS_PATH):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    os.makedirs(db_path, exist_ok=True)

    with open(data_path, "r", encoding="utf-8") as f:
        full_text = f.read()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    chunks = splitter.split_text(full_text)

    old_hashes = {}
    if os.path.exists(HASH_PATH):
        with open(HASH_PATH, "r", encoding="utf-8") as f:
            old_hashes = json.load(f)

    new_chunks = []
    new_hashes = {}
    for chunk in chunks:
        h = compute_md5(chunk)
        new_hashes[h] = True
        if h not in old_hashes:
            new_chunks.append(chunk)

    if not os.path.exists(os.path.join(db_path, "index.faiss")):
        st.info("üß† T·∫°o m·ªõi FAISS database...")
        vector_store = FAISS.from_texts(chunks, embedding=embeddings)
    else:
        vector_store = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        if new_chunks:
            st.info(f"üîÑ Ph√°t hi·ªán {len(new_chunks)} ƒëo·∫°n m·ªõi ‚Üí c·∫≠p nh·∫≠t FAISS...")
            vector_store.add_texts(new_chunks)
        else:
            st.success("‚úÖ Kh√¥ng c√≥ thay ƒë·ªïi m·ªõi trong c∆° s·ªü d·ªØ li·ªáu.")

    vector_store.save_local(db_path)
    with open(HASH_PATH, "w", encoding="utf-8") as f:
        json.dump(new_hashes, f, ensure_ascii=False, indent=2)

    st.success("‚úÖ FAISS ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t.")
    return vector_store

# =============================
# üîπ RAG QA Chain
# =============================
def create_qa_chain(vector_store):
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})
    prompt_template = """
    B·∫°n l√† m·ªôt **chuy√™n gia y h·ªçc c·ªï truy·ªÅn Vi·ªát Nam** am hi·ªÉu d∆∞·ª£c li·ªáu v√† b√†i thu·ªëc b·∫Øc.
    Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc k·ªπ ng·ªØ c·∫£nh v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin c√≥ trong ƒë√≥.

    üîπ Quy t·∫Øc tr·∫£ l·ªùi:
    1. Gi·∫£i th√≠ch ng·∫Øn g·ªçn (2‚Äì3 c√¢u t·ªïng qu√°t).
    2. Li·ªát k√™ chi ti·∫øt c√°c **b√†i thu·ªëc** li√™n quan (n·∫øu c√≥):
       - **T√™n b√†i thu·ªëc:** ...
       - **Th√†nh ph·∫ßn:** ...
       - **C√°ch d√πng:** ...
       - **Ch·ªâ ƒë·ªãnh:** ...
    3. N·∫øu c√≥ nhi·ªÅu b√†i thu·ªëc, s·∫Øp x·∫øp theo ƒë·ªô ph√π h·ª£p cao nh·∫•t.
    4. N·∫øu kh√¥ng c√≥ th√¥ng tin v·ªÅ b√†i thu·ªëc, ƒë∆∞a ra b√†i thu·ªëc c√≥ s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ cao nh·∫•t
    5. Cu·ªëi c√πng, th√™m ghi ch√∫: *(Th√¥ng tin ch·ªâ mang t√≠nh tham kh·∫£o, kh√¥ng thay th·∫ø t∆∞ v·∫•n y khoa).*
    6. N·∫øu kh√¥ng t√¨m th·∫•y, tr·∫£ l·ªùi: "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu."
    -----------------
    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}
    -----------------
    Tr·∫£ l·ªùi:
    """

    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=prompt_template
    )

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )

    return qa_chain

# =============================
# üîπ Giao di·ªán Streamlit
# =============================
def main():
    st.set_page_config(page_title="üåø Chatbot Thu·ªëc B·∫Øc", page_icon="üåø", layout="wide")
    st.title("üåø Chatbot Y h·ªçc C·ªï truy·ªÅn ‚Äì Nh·∫≠n di·ªán & G·ª£i √Ω b√†i thu·ªëc")

    # 1Ô∏è‚É£ T·∫£i database
    if not os.path.exists(DATA_PATH):
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file {DATA_PATH}. H√£y ƒë·∫∑t n√≥ c√πng th∆∞ m·ª•c ·ª©ng d·ª•ng.")
        st.stop()

    st.info("üìÇ ƒêang t·∫£i ho·∫∑c c·∫≠p nh·∫≠t FAISS database t·ª´ text.txt...")
    vector_store = load_or_update_faiss(DATA_PATH, FAISS_PATH)

    # 2Ô∏è‚É£ Sidebar: nh·∫≠p ·∫£nh
    st.sidebar.header("üì∏ Nh·∫≠n di·ªán v·ªã thu·ªëc")
    uploaded_files = st.sidebar.file_uploader(
        "T·∫£i ·∫£nh v·ªã thu·ªëc (PNG, JPG, JPEG, BMP):",
        accept_multiple_files=True,
        type=["png", "jpg", "jpeg", "bmp"]
    )

    # 3Ô∏è‚É£ Model YOLO (cache)
    @st.cache_resource
    def load_recognition_model():
        return YOLO(r'C:/Users/admin/OneDrive/Desktop/NCKH/recog.pt')

    recognition_model = load_recognition_model()

    # 4Ô∏è‚É£ Nh·∫≠p c√¢u h·ªèi
    st.markdown("### üí¨ Nh·∫≠p c√¢u h·ªèi (ho·∫∑c ƒë·ªÉ tr·ªëng n·∫øu ch·ªâ mu·ªën nh·∫≠n di·ªán ·∫£nh)")
    user_question = st.text_input("V√≠ d·ª•: 'B√†i thu·ªëc n√†o tr·ªã m·∫•t ng·ªß c√≥ v·ªã k·ª∑ t·ª≠ v√† ho√†ng k·ª≥?'")

    # 5Ô∏è‚É£ N√∫t th·ª±c thi
    if st.button("üîç Ph√¢n t√≠ch & G·ª£i √Ω b√†i thu·ªëc"):
        detected_herbs = ""

        # --- N·∫øu c√≥ ·∫£nh, nh·∫≠n di·ªán ---
        if uploaded_files:
            for file in uploaded_files:
                image = Image.open(io.BytesIO(file.read()))
                st.image(image, caption=f"·∫¢nh: {file.name}", use_container_width=True)

                with st.spinner("üîé ƒêang nh·∫≠n di·ªán v·ªã thu·ªëc..."):
                    result = recognition_model.predict(image)
                    names = recognition_model.names
                    for r in result:
                        for box in r.boxes:
                            cls = int(box.cls)
                            herb_name = names[cls]
                            if herb_name not in detected_herbs:
                                detected_herbs += herb_name + ", "
                                st.success(f"‚úÖ Ph√°t hi·ªán: **{herb_name}**")
                            str+=names[cls] +' , '
                            
            if str !='':
                st.markdown("### üßæ K·∫øt qu·∫£ nh·∫≠n di·ªán:")
                st.write(str)
        # --- G·ªôp c√¢u h·ªèi ---
        if detected_herbs:
            query = f"C√°c b√†i thu·ªëc c√≥ ch·ª©a {detected_herbs.strip(', ')}"
            if user_question:
                query += f". {user_question}"
        else:
            if not user_question:
                st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i ·∫£nh ho·∫∑c nh·∫≠p c√¢u h·ªèi.")
                st.stop()
            query = user_question

        # --- RAG truy v·∫•n ---
        with st.spinner("üßò‚Äç‚ôÇÔ∏è ƒêang tra c·ª©u b√†i thu·ªëc..."):
            qa_chain = create_qa_chain(vector_store)
            result = qa_chain({"query": query})

        # --- K·∫øt qu·∫£ ---
        st.success("‚úÖ Ho√†n t·∫•t.")
        st.markdown("### üß† K·∫øt qu·∫£:")
        st.markdown(result["result"])


if __name__ == "__main__":
    main()


