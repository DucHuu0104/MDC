import os
import hashlib
import json
import io
import streamlit as st
from dotenv import load_dotenv
from PIL import Image
from ultralytics import YOLO
# LangChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate

# ======================================================
# âš™ï¸ Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng
# ======================================================
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âŒ KhÃ´ng tÃ¬m tháº¥y API key trong file .env")
    st.stop()

DATA_PATH = "text.txt"
FAISS_PATH = "faiss_index"
HASH_PATH = os.path.join(FAISS_PATH, "data_hash.json")

# ======================================================
# ğŸ”¹ HÃ m tiá»‡n Ã­ch
# ======================================================
def compute_md5(text):
    return hashlib.md5(text.encode("utf-8")).hexdigest()

# ======================================================
# ğŸ”¹ FAISS loader/update
# ======================================================
def load_or_update_faiss(data_path=DATA_PATH, db_path=FAISS_PATH):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    os.makedirs(db_path, exist_ok=True)

    with open(data_path, "r", encoding="utf-8") as f:
        full_text = f.read()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    chunks = splitter.split_text(full_text)

    old_hashes = {}
    if os.path.exists(HASH_PATH):
        with open(HASH_PATH, "r", encoding="utf-8") as f:
            old_hashes = json.load(f)

    new_chunks = []
    new_hashes = {}
    for chunk in chunks:
        h = compute_md5(chunk)
        new_hashes[h] = True
        if h not in old_hashes:
            new_chunks.append(chunk)

    if not os.path.exists(os.path.join(db_path, "index.faiss")):
        st.info("ğŸ§  Táº¡o má»›i FAISS database...")
        vector_store = FAISS.from_texts(chunks, embedding=embeddings)
    else:
        vector_store = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
        if new_chunks:
            st.info(f"ğŸ”„ PhÃ¡t hiá»‡n {len(new_chunks)} Ä‘oáº¡n má»›i â†’ cáº­p nháº­t FAISS...")
            vector_store.add_texts(new_chunks)
        else:
            st.success("âœ… KhÃ´ng cÃ³ thay Ä‘á»•i má»›i trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.")

    vector_store.save_local(db_path)
    with open(HASH_PATH, "w", encoding="utf-8") as f:
        json.dump(new_hashes, f, ensure_ascii=False, indent=2)

    st.success("âœ… FAISS Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t.")
    return vector_store

# ======================================================
# ğŸ”¹ RAG QA Chain (theo k chunk)
# ======================================================
def create_qa_chain(vector_store, k=5):
    retriever = vector_store.as_retriever(search_kwargs={"k": k})
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3)

    prompt_template = """
    Báº¡n lÃ  **chuyÃªn gia y há»c cá»• truyá»n Viá»‡t Nam**, am hiá»ƒu dÆ°á»£c liá»‡u vÃ  bÃ i thuá»‘c báº¯c.
    Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘á»c ká»¹ NGá»® Cáº¢NH vÃ  tráº£ lá»i CÃ‚U Há»I dá»±a trÃªn thÃ´ng tin cÃ³ trong Ä‘Ã³.

    ğŸ”¹ Quy táº¯c tráº£ lá»i:
    1. Giáº£i thÃ­ch ngáº¯n gá»n (2â€“3 cÃ¢u tá»•ng quÃ¡t).
    2. Liá»‡t kÃª chi tiáº¿t cÃ¡c **bÃ i thuá»‘c** liÃªn quan (náº¿u cÃ³):
       - **TÃªn bÃ i thuá»‘c:** ...
       - **ThÃ nh pháº§n:** ...
       - **CÃ¡ch dÃ¹ng:** ...
       - **Chá»‰ Ä‘á»‹nh:** ...
    3. Náº¿u cÃ³ nhiá»u bÃ i thuá»‘c, sáº¯p xáº¿p theo Ä‘á»™ phÃ¹ há»£p cao nháº¥t.
    4. Náº¿u khÃ´ng cÃ³ thÃ´ng tin vá» bÃ i thuá»‘c, Ä‘Æ°a ra bÃ i thuá»‘c cÃ³ sá»‘ lÆ°á»£ng thÃ nh pháº§n cao nháº¥t.
    5. Cuá»‘i cÃ¹ng, thÃªm ghi chÃº: *(ThÃ´ng tin chá»‰ mang tÃ­nh tham kháº£o, khÃ´ng thay tháº¿ tÆ° váº¥n y khoa).*
    6. Náº¿u khÃ´ng tÃ¬m tháº¥y, tráº£ lá»i: "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong tÃ i liá»‡u."

    -----------------
    Ngá»¯ cáº£nh (tá»« {k} Ä‘oáº¡n gáº§n nháº¥t):
    {context}
    -----------------
    CÃ¢u há»i: {question}
    -----------------
    Tráº£ lá»i:
    """

    prompt = PromptTemplate(
        input_variables=["context", "question", "k"],
        template=prompt_template
    )

    def ask_with_top_k(query):
        # ğŸ”¹ Láº¥y ra k Ä‘oáº¡n gáº§n nháº¥t
        docs = retriever.get_relevant_documents(query)
        top_k_docs = docs[:k]
        context = "\n\n".join([d.page_content for d in top_k_docs])

        # ğŸ”¹ Gá»i LLM trá»±c tiáº¿p vá»›i prompt Ä‘Ã£ ghÃ©p
        final_prompt = prompt.format(context=context, question=query, k=k)
        response = llm.invoke(final_prompt)
        return response.content

    return ask_with_top_k

# ======================================================
# ğŸ”¹ Giao diá»‡n Streamlit
# ======================================================
def main():
    st.set_page_config(page_title="ğŸŒ¿ Chatbot Thuá»‘c Báº¯c", page_icon="ğŸŒ¿", layout="wide")
    st.title("ğŸŒ¿ Chatbot Y há»c Cá»• truyá»n â€“ Nháº­n diá»‡n & Gá»£i Ã½ bÃ i thuá»‘c")

    # 1ï¸âƒ£ Khá»Ÿi táº¡o cÆ¡ sá»Ÿ dá»¯ liá»‡u
    if not os.path.exists(DATA_PATH):
        st.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file {DATA_PATH}. HÃ£y Ä‘áº·t nÃ³ cÃ¹ng thÆ° má»¥c á»©ng dá»¥ng.")
        st.stop()

    st.info("ğŸ“‚ Äang táº£i hoáº·c cáº­p nháº­t FAISS database tá»« text.txt...")
    vector_store = load_or_update_faiss(DATA_PATH, FAISS_PATH)

    # 2ï¸âƒ£ Sidebar: Chá»n sá»‘ chunk vÃ  táº£i áº£nh
    st.sidebar.header("âš™ï¸ CÃ i Ä‘áº·t")
    k = st.sidebar.slider("ğŸ”¢ Sá»‘ Ä‘oáº¡n vÄƒn (chunk) sá»­ dá»¥ng:", 1, 10, 5)

    st.sidebar.header("ğŸ“¸ Nháº­n diá»‡n vá»‹ thuá»‘c")
    uploaded_files = st.sidebar.file_uploader(
        "Táº£i áº£nh vá»‹ thuá»‘c (PNG, JPG, JPEG, BMP):",
        accept_multiple_files=True,
        type=["png", "jpg", "jpeg", "bmp"]
    )

    # 3ï¸âƒ£ Khá»Ÿi táº¡o model YOLO nháº­n dáº¡ng thuá»‘c
    @st.cache_resource
    def load_recognition_model():
        return YOLO(r'C:/Users/admin/OneDrive/Desktop/NCKH/recog.pt')

    recognition_model = load_recognition_model()

    # 4ï¸âƒ£ Nháº­p cÃ¢u há»i vÄƒn báº£n
    st.markdown("### ğŸ’¬ Há»i trá»±c tiáº¿p vá» bÃ i thuá»‘c / triá»‡u chá»©ng")
    user_question = st.text_input("VÃ­ dá»¥: 'BÃ i thuá»‘c nÃ o trá»‹ máº¥t ngá»§ cÃ³ vá»‹ ká»· tá»­ vÃ  hoÃ ng ká»³?'")

    

    # 5ï¸âƒ£ Nháº­n dáº¡ng thuá»‘c tá»« áº£nh
    st.markdown("---")
    st.markdown("### ğŸ§ª Káº¿t quáº£ nháº­n diá»‡n thuá»‘c")

    if st.sidebar.button("ğŸ” Nháº­n diá»‡n & Gá»£i Ã½ bÃ i thuá»‘c"):
        if not uploaded_files:
            st.sidebar.error("âŒ Vui lÃ²ng táº£i Ã­t nháº¥t má»™t áº£nh.")
        else:
            detected_herbs = []

            for file in uploaded_files:
                image = Image.open(io.BytesIO(file.read()))
                st.image(image, caption=f"áº¢nh: {file.name}", use_container_width=True)

                with st.spinner("ğŸ” Äang nháº­n diá»‡n vá»‹ thuá»‘c..."):
                    result = recognition_model.predict(image)
                    names = recognition_model.names  # dict id -> name
                    for r in result:
                        for box in r.boxes:
                            cls = int(box.cls)
                            name = names[cls]
                            if name not in detected_herbs:
                                detected_herbs.append(name)
                                st.success(f"âœ… PhÃ¡t hiá»‡n: **{name}**")

            if detected_herbs:
                
                herb_list = ", ".join(detected_herbs)
                st.markdown("### ğŸ§¾ Káº¿t quáº£ nháº­n diá»‡n:")
                st.write(herb_list)

                # ğŸ” Tá»± Ä‘á»™ng truy váº¥n RAG Ä‘á»ƒ gá»£i Ã½ bÃ i thuá»‘c
                st.markdown("### ğŸ’¬ Gá»£i Ã½ bÃ i thuá»‘c liÃªn quan:")
                
                query = f"CÃ¡c bÃ i thuá»‘c cÃ³ chá»©a: {herb_list} dá»±a trÃªn ngá»¯ cáº£nh {user_question} "
                qa_chain = create_qa_chain(vector_store, k=k)
                result = qa_chain(query)
                st.markdown(result)

# ğŸš€ Run app
if __name__ == "__main__":
    main()
